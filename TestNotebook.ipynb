{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tables\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import fnmatch\n",
    "import os\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers, regularizers\n",
    "from tensorflow.keras.layers import Input, Concatenate, concatenate, Dense,Embedding, Conv2D, Conv3D, MaxPooling2D, MaxPooling3D, Flatten, Dropout, ConvLSTM2D, BatchNormalization\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "import glob\n",
    "import pickle\n",
    "import sys\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctapipe.io import EventSource\n",
    "from ctapipe import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filePath=\"../../../mnt/c/Users/hanne/Desktop/Studium Physik/ECAP_HiWi_CNN/ECAP_HiWi_WorkingDirectory/phase2d3_timeinfo_gamma_diffuse_hybrid_preselect_20deg_0deg.h5\"\n",
    "X = tables.open_file(filePath, mode=\"r\")\n",
    "#print(X.get_node('/dl1/event/telescope/images/tel_001').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(960,)\n"
     ]
    }
   ],
   "source": [
    "Sample_Image_tel_001 =  X.get_node('/dl1/event/telescope/images/tel_001').read()[30][3]\n",
    "print(np.shape(Sample_Image_tel_001))\n",
    "#print(X.get_node('/dl1/event/telescope/images/tel_002').read()[40][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(Sample_Image_tel_001)\n",
    "#import dl1_data_handler\n",
    "#grid = Sample_Image_tel_001.reshape((31,31))\n",
    "#plt.imshow(grid, cmap='viridis')\n",
    "#plt.colorbar()\n",
    "#plt.show()\n",
    "from ctapipe.instrument.camera import CameraGeometry\n",
    "\n",
    "from dl1_data_handler.reader import DL1DataReader\n",
    "from dl1_data_handler.image_mapper import ImageMapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "``/mnt/c/Users/hanne/Desktop/Studium Physik/ECAP_HiWi_CNN/dl1_data_handler_demo/phase2d3_timeinfo_gamma_diffuse_hybrid_preselect_20deg_0deg.h5`` does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#reader = DL1DataReader([\"../../../mnt/c/Users/hanne/Desktop/Studium Physik/ECAP_HiWi_CNN/dl1_data_handler_demo/gamma_20deg_0deg_runs100-103___cta-prod3-demo_desert-2150m-Paranal-baseline-sample_cone10.h5\"])\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m reader \u001b[39m=\u001b[39m DL1DataReader([\u001b[39m\"\u001b[39;49m\u001b[39m../../../mnt/c/Users/hanne/Desktop/Studium Physik/ECAP_HiWi_CNN/dl1_data_handler_demo/phase2d3_timeinfo_gamma_diffuse_hybrid_preselect_20deg_0deg.h5\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[0;32m~/miniconda3/envs/HESSML_ENV2/lib/python3.10/site-packages/dl1_data_handler/reader.py:53\u001b[0m, in \u001b[0;36mDL1DataReader.__init__\u001b[0;34m(self, file_list, mode, subarray_info, event_info)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mfor\u001b[39;00m filename \u001b[39min\u001b[39;00m file_list:\n\u001b[1;32m     52\u001b[0m     \u001b[39mwith\u001b[39;00m lock:\n\u001b[0;32m---> 53\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfiles[filename] \u001b[39m=\u001b[39m tables\u001b[39m.\u001b[39;49mopen_file(filename, mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     55\u001b[0m \u001b[39m# Save the user attributes for the first file\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_v_attrs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfiles[\u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfiles)[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39mroot\u001b[39m.\u001b[39m_v_attrs\n",
      "File \u001b[0;32m~/miniconda3/envs/HESSML_ENV2/lib/python3.10/site-packages/tables/file.py:300\u001b[0m, in \u001b[0;36mopen_file\u001b[0;34m(filename, mode, title, root_uep, filters, **kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    296\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mThe file \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is already opened.  Please \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mclose it before reopening in write mode.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m filename)\n\u001b[1;32m    299\u001b[0m \u001b[39m# Finally, create the File instance, and return it\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m \u001b[39mreturn\u001b[39;00m File(filename, mode, title, root_uep, filters, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/HESSML_ENV2/lib/python3.10/site-packages/tables/file.py:750\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, filename, mode, title, root_uep, filters, **kwargs)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams \u001b[39m=\u001b[39m params\n\u001b[1;32m    749\u001b[0m \u001b[39m# Now, it is time to initialize the File extension\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_g_new(filename, mode, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    752\u001b[0m \u001b[39m# Check filters and set PyTables format version for new files.\u001b[39;00m\n\u001b[1;32m    753\u001b[0m new \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_v_new\n",
      "File \u001b[0;32m~/miniconda3/envs/HESSML_ENV2/lib/python3.10/site-packages/tables/hdf5extension.pyx:368\u001b[0m, in \u001b[0;36mtables.hdf5extension.File._g_new\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/HESSML_ENV2/lib/python3.10/site-packages/tables/utils.py:143\u001b[0m, in \u001b[0;36mcheck_file_access\u001b[0;34m(filename, mode)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    141\u001b[0m     \u001b[39m# The file should be readable.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39maccess(path, os\u001b[39m.\u001b[39mF_OK):\n\u001b[0;32m--> 143\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m``\u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m`` does not exist\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m path\u001b[39m.\u001b[39mis_file():\n\u001b[1;32m    145\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m``\u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m`` is not a regular file\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: ``/mnt/c/Users/hanne/Desktop/Studium Physik/ECAP_HiWi_CNN/dl1_data_handler_demo/phase2d3_timeinfo_gamma_diffuse_hybrid_preselect_20deg_0deg.h5`` does not exist"
     ]
    }
   ],
   "source": [
    "#reader = DL1DataReader([\"../../../mnt/c/Users/hanne/Desktop/Studium Physik/ECAP_HiWi_CNN/dl1_data_handler_demo/gamma_20deg_0deg_runs100-103___cta-prod3-demo_desert-2150m-Paranal-baseline-sample_cone10.h5\"])\n",
    "reader = DL1DataReader([\"../../../mnt/c/Users/hanne/Desktop/Studium Physik/ECAP_HiWi_CNN/dl1_data_handler_demo/phase2d3_timeinfo_gamma_diffuse_hybrid_preselect_20deg_0deg.h5\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_append_subarray_info', '_construct_simulated_info', '_construct_unprocessed_example_description', '_get_camera_type', '_get_image', '_v_attrs', 'class_weight', 'event_info', 'files', 'mode', 'shower_primary_id_to_name', 'subarray_info']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DL1DataReader' object has no attribute 'example_description'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m reader \u001b[39m=\u001b[39m DL1DataReader([\u001b[39m\"\u001b[39m\u001b[39m../../../mnt/c/Users/hanne/Desktop/Studium Physik/ECAP_HiWi_CNN/ECAP_HiWi_WorkingDirectory/phase2d3_timeinfo_gamma_diffuse_hybrid_preselect_20deg_0deg.h5\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mdir\u001b[39m(reader))\n\u001b[0;32m----> 3\u001b[0m \u001b[39mprint\u001b[39m(reader\u001b[39m.\u001b[39;49mexample_description)\n\u001b[1;32m      4\u001b[0m \u001b[39m#print(\"Image shape: {}\".format(reader[0][0].shape))\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[39m#print(\"Image shape: {}\".format(reader[0][0].shape))\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m#print(reader.get_node('/dl1/event/telescope/images/tel_001').read()[30][3])\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DL1DataReader' object has no attribute 'example_description'"
     ]
    }
   ],
   "source": [
    "reader = DL1DataReader([\"../../../mnt/c/Users/hanne/Desktop/Studium Physik/ECAP_HiWi_CNN/ECAP_HiWi_WorkingDirectory/phase2d3_timeinfo_gamma_diffuse_hybrid_preselect_20deg_0deg.h5\"])\n",
    "print(dir(reader))\n",
    "print(reader.example_description)\n",
    "#print(\"Image shape: {}\".format(reader[0][0].shape))\n",
    "\n",
    "#print(\"Image shape: {}\".format(reader[0][0].shape))\n",
    "#print(reader.get_node('/dl1/event/telescope/images/tel_001').read()[30][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the camera types and mapping methods\n",
    "hex_cams = ['LSTCam', 'FlashCam', 'NectarCam', 'DigiCam', 'VERITAS',\n",
    "            'MAGICCam', 'FACT', 'HESS-I', 'HESS-II']\n",
    "square_cams = ['SCTCam', 'CHEC', 'ASTRICam']\n",
    "camera_types = hex_cams + square_cams\n",
    "hex_methods = ['oversampling', 'rebinning', 'nearest_interpolation',\n",
    "               'bilinear_interpolation', 'bicubic_interpolation', \n",
    "               'image_shifting', 'axial_addressing']\n",
    "square_methods = ['oversampling', 'rebinning', 'nearest_interpolation',\n",
    "                  'bilinear_interpolation', 'bicubic_interpolation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization time (total for all telescopes):\n",
      "oversampling\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find any file: LSTSiPMCam.camgeom[.fits.gz, .fits, .ecsv, .ecsv.txt]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m cam \u001b[39min\u001b[39;00m square_cams:\n\u001b[1;32m      8\u001b[0m     mapping_method[cam] \u001b[39m=\u001b[39m method \u001b[39mif\u001b[39;00m method \u001b[39min\u001b[39;00m square_methods \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39moversampling\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 9\u001b[0m get_ipython()\u001b[39m.\u001b[39;49mrun_line_magic(\u001b[39m'\u001b[39;49m\u001b[39mtimeit\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mmappers[method] = ImageMapper(mapping_method=mapping_method)\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/HESSML_ENV2/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2417\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2415\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mlocal_ns\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_local_scope(stack_depth)\n\u001b[1;32m   2416\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 2417\u001b[0m     result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2419\u001b[0m \u001b[39m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2420\u001b[0m \u001b[39m# when using magics with decodator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2421\u001b[0m \u001b[39m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2422\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(fn, magic\u001b[39m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[39mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/miniconda3/envs/HESSML_ENV2/lib/python3.10/site-packages/IPython/core/magics/execution.py:1170\u001b[0m, in \u001b[0;36mExecutionMagics.timeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1168\u001b[0m \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m10\u001b[39m):\n\u001b[1;32m   1169\u001b[0m     number \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m index\n\u001b[0;32m-> 1170\u001b[0m     time_number \u001b[39m=\u001b[39m timer\u001b[39m.\u001b[39;49mtimeit(number)\n\u001b[1;32m   1171\u001b[0m     \u001b[39mif\u001b[39;00m time_number \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.2\u001b[39m:\n\u001b[1;32m   1172\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/HESSML_ENV2/lib/python3.10/site-packages/IPython/core/magics/execution.py:158\u001b[0m, in \u001b[0;36mTimer.timeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    156\u001b[0m gc\u001b[39m.\u001b[39mdisable()\n\u001b[1;32m    157\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     timing \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner(it, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimer)\n\u001b[1;32m    159\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    160\u001b[0m     \u001b[39mif\u001b[39;00m gcold:\n",
      "File \u001b[0;32m<magic-timeit>:1\u001b[0m, in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/HESSML_ENV2/lib/python3.10/site-packages/dl1_data_handler/image_mapper.py:93\u001b[0m, in \u001b[0;36mImageMapper.__init__\u001b[0;34m(self, camera_types, pixel_positions, mapping_method, padding, interpolation_image_shape, mask_interpolation)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m     91\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe `ctapipe.instrument.camera` python module is required, if pixel_positions is `None`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m     )\n\u001b[0;32m---> 93\u001b[0m camgeo \u001b[39m=\u001b[39m CameraGeometry\u001b[39m.\u001b[39;49mfrom_name(camtype)\n\u001b[1;32m     94\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_pixels[camtype] \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(camgeo\u001b[39m.\u001b[39mpix_id)\n\u001b[1;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpixel_positions[camtype] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mcolumn_stack(\n\u001b[1;32m     96\u001b[0m     [camgeo\u001b[39m.\u001b[39mpix_x\u001b[39m.\u001b[39mvalue, camgeo\u001b[39m.\u001b[39mpix_y\u001b[39m.\u001b[39mvalue]\n\u001b[1;32m     97\u001b[0m )\u001b[39m.\u001b[39mT\n",
      "File \u001b[0;32m~/miniconda3/envs/HESSML_ENV2/lib/python3.10/site-packages/ctapipe/instrument/camera/geometry.py:584\u001b[0m, in \u001b[0;36mCameraGeometry.from_name\u001b[0;34m(cls, name, version)\u001b[0m\n\u001b[1;32m    581\u001b[0m     verstr \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m{\u001b[39;00mversion\u001b[39m:\u001b[39;00m\u001b[39m03d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m tabname \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m{name}\u001b[39;00m\u001b[39m{verstr}\u001b[39;00m\u001b[39m.camgeom\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname, verstr\u001b[39m=\u001b[39mverstr)\n\u001b[0;32m--> 584\u001b[0m table \u001b[39m=\u001b[39m get_table_dataset(tabname, role\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdl0.tel.svc.camera\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    585\u001b[0m \u001b[39mreturn\u001b[39;00m CameraGeometry\u001b[39m.\u001b[39mfrom_table(table)\n",
      "File \u001b[0;32m~/miniconda3/envs/HESSML_ENV2/lib/python3.10/site-packages/ctapipe/utils/datasets.py:302\u001b[0m, in \u001b[0;36mget_table_dataset\u001b[0;34m(table_name, role, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[39m# a mapping of types (keys) to any extra keyword args needed for\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[39m# table.read()\u001b[39;00m\n\u001b[1;32m    295\u001b[0m table_types \u001b[39m=\u001b[39m {\n\u001b[1;32m    296\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m.fits.gz\u001b[39m\u001b[39m\"\u001b[39m: Table\u001b[39m.\u001b[39mread,\n\u001b[1;32m    297\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m.fits\u001b[39m\u001b[39m\"\u001b[39m: Table\u001b[39m.\u001b[39mread,\n\u001b[1;32m    298\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m.ecsv\u001b[39m\u001b[39m\"\u001b[39m: partial(Table\u001b[39m.\u001b[39mread, \u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mascii.ecsv\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m    299\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m.ecsv.txt\u001b[39m\u001b[39m\"\u001b[39m: partial(Table\u001b[39m.\u001b[39mread, \u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mascii.ecsv\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m    300\u001b[0m }\n\u001b[0;32m--> 302\u001b[0m \u001b[39mreturn\u001b[39;00m try_filetypes(table_name, role, table_types, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/HESSML_ENV2/lib/python3.10/site-packages/ctapipe/utils/datasets.py:268\u001b[0m, in \u001b[0;36mtry_filetypes\u001b[0;34m(basename, role, file_types, url, **kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     Provenance()\u001b[39m.\u001b[39madd_input_file(path, role)\n\u001b[1;32m    266\u001b[0m     \u001b[39mreturn\u001b[39;00m table\n\u001b[0;32m--> 268\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m    269\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find any file: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m[\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(basename, \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(file_types))\n\u001b[1;32m    270\u001b[0m )\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find any file: LSTSiPMCam.camgeom[.fits.gz, .fits, .ecsv, .ecsv.txt]"
     ]
    }
   ],
   "source": [
    "# Load the image mappers\n",
    "mappers = {}\n",
    "print(\"Initialization time (total for all telescopes):\")\n",
    "for method in hex_methods:\n",
    "    print(method)\n",
    "    mapping_method = {cam: method for cam in hex_cams}\n",
    "    for cam in square_cams:\n",
    "        mapping_method[cam] = method if method in square_methods else 'oversampling'\n",
    "    %timeit mappers[method] = ImageMapper(mapping_method=mapping_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DataContainer' from 'ctapipe.containers' (/home/hanneswarnhofer/miniconda3/envs/HESSML_ENV2/lib/python3.10/site-packages/ctapipe/containers.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdl1_data_handler\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwriter\u001b[39;00m \u001b[39mimport\u001b[39;00m DL1DataWriter, CTAMLDataDumper\n",
      "File \u001b[0;32m~/miniconda3/envs/HESSML_ENV2/lib/python3.10/site-packages/dl1_data_handler/writer.py:29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mctapipe\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minstrument\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcamera\u001b[39;00m \u001b[39mimport\u001b[39;00m CameraGeometry\n\u001b[1;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdl1_data_handler\u001b[39;00m \u001b[39mimport\u001b[39;00m table_definitions \u001b[39mas\u001b[39;00m table_defs\n\u001b[0;32m---> 29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdl1_data_handler\u001b[39;00m \u001b[39mimport\u001b[39;00m dl_eventsources\n\u001b[1;32m     31\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m__name__\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mDL1DataDumper\u001b[39;00m(ABC):\n",
      "File \u001b[0;32m~/miniconda3/envs/HESSML_ENV2/lib/python3.10/site-packages/dl1_data_handler/dl_eventsources.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mastropy\u001b[39;00m \u001b[39mimport\u001b[39;00m units \u001b[39mas\u001b[39;00m u\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mastropy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcoordinates\u001b[39;00m \u001b[39mimport\u001b[39;00m Angle\n\u001b[0;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mctapipe\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcontainers\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     DataContainer,\n\u001b[1;32m     11\u001b[0m     TelescopePointingContainer,\n\u001b[1;32m     12\u001b[0m     LeakageContainer,\n\u001b[1;32m     13\u001b[0m     HillasParametersContainer,\n\u001b[1;32m     14\u001b[0m     ConcentrationContainer,\n\u001b[1;32m     15\u001b[0m     TimingParametersContainer,\n\u001b[1;32m     16\u001b[0m     MorphologyContainer,\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mctapipe\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minstrument\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     19\u001b[0m     TelescopeDescription,\n\u001b[1;32m     20\u001b[0m     SubarrayDescription,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     CameraDescription,\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mctapipe\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meventsource\u001b[39;00m \u001b[39mimport\u001b[39;00m EventSource\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'DataContainer' from 'ctapipe.containers' (/home/hanneswarnhofer/miniconda3/envs/HESSML_ENV2/lib/python3.10/site-packages/ctapipe/containers.py)"
     ]
    }
   ],
   "source": [
    "\n",
    "from dl1_data_handler.writer import DL1DataWriter, CTAMLDataDumper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-n NUMBERS] [-e EPOCHS] [-b BATCH_SIZE]\n",
      "                             [-r RATE] [-reg REGULIZATION]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9027 --control=9025 --hb=9024 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"a10f8126-f77a-4153-9488-6fdf85c05b9f\" --shell=9026 --transport=\"tcp\" --iopub=9028 --f=/home/hanneswarnhofer/.local/share/jupyter/runtime/kernel-v2-16483zh33QPgee3h5.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanneswarnhofer/anaconda3/envs/HESSML_ENV/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3513: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Author:\n",
    "# Hannes Warnhofer\n",
    "# 22715256\n",
    "# hannes.warnhofer@fau.de\n",
    "\n",
    "# Test if GitHub Connection works as intended\n",
    "lll = 1\n",
    "# import relevant (and irrelevant) packages \n",
    "import tables\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import fnmatch\n",
    "import os\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers, regularizers\n",
    "from tensorflow.keras.layers import Input, Concatenate, concatenate, Dense,Embedding, Conv2D, Conv3D, MaxPooling2D, MaxPooling3D, Flatten, Dropout, ConvLSTM2D, BatchNormalization\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "import glob\n",
    "import pickle\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "# Allow for arguments to be passed to the python file, when running it. This enables to define a series of runs with different model properties, by defining this in the respective batch script.\n",
    "# Example for usage: srun python /home/hpc/b129dc/b129dc26/MoDA_Project/Hannes_Warnhofer_22715256_CTA_Multiview_Analysis_Cluster.py -n \"345\" -e 50 -b 512 -r 0.25 -reg 0.001\n",
    "# This uses all the data that starts with 3,4 or 5, runs over 50 epochs, has a batch size of 512 and a dropout rate of .25 and a regularization of 0.001\n",
    "# The following installations have to be done when running a batch script on Alex:\n",
    "# module load python/tensorflow-2.7.0py3.9\n",
    "# pip install --user tables\n",
    "# pip install --user pandas\n",
    "# pip install --user pickle\n",
    "# pip install --user glob\n",
    "# pip install --user sys\n",
    "# pip install --user argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-n\", \"--numbers\", type=str)\n",
    "parser.add_argument(\"-e\", \"--epochs\", type=int)\n",
    "parser.add_argument(\"-b\", \"--batch_size\", type=int)\n",
    "parser.add_argument(\"-r\", \"--rate\", type=float)\n",
    "parser.add_argument(\"-reg\", \"--regulization\", type=float)\n",
    "\n",
    "args = parser.parse_args()\n",
    "var1 = args.numbers\n",
    "num_epochs = args.epochs\n",
    "batch_size = args.batch_size\n",
    "rate = args.rate\n",
    "reg = args.regulization\n",
    "patience = 5\n",
    "\n",
    "# Define the appendix to the file, for being able to specify some general changes in the model structure and trace back the changes when comparing the results of tÂ´different models\n",
    "fnr = \"_2023-02-27_\" + var1 +'_only_earlystopping'\n",
    "\n",
    "# Create file paths from the argument that specifies which data files to include\n",
    "crit = '[' + var1 + ']*.hdf5'\n",
    "file_paths = [f for f in glob.glob(os.path.join(cluster_filePath, crit))]\n",
    "\n",
    "\n",
    "# load in the data\n",
    "####################################\n",
    "\n",
    "# define empty arrays\n",
    "squared_training = []\n",
    "peak_times = []\n",
    "event_labels = []\n",
    "\n",
    "# go through the file_paths of the includes files\n",
    "for i in file_paths:\n",
    "    with tables.open_file(i, mode=\"r\") as x:\n",
    "        # ignore the electron events\n",
    "        mask_e = x.root.event_label[:] != 2 \n",
    "        event_labels_temp = x.root.event_label[:][mask_e].reshape((-1, 1))\n",
    "        squared_training_temp = x.root.squared_training[:,:,:,:][mask_e]\n",
    "        peak_times_temp = x.root.peak_times[:,:,:,:][mask_e]\n",
    "\n",
    "        # append all the data to a common array\n",
    "        squared_training.append(squared_training_temp)\n",
    "        peak_times.append(peak_times_temp)\n",
    "        event_labels.append(event_labels_temp)\n",
    "\n",
    "squared_training = np.concatenate(squared_training, axis=0)\n",
    "peak_times = np.concatenate(peak_times, axis=0)\n",
    "event_labels = np.concatenate(event_labels, axis=0)\n",
    "\n",
    "# some reshaping for the further use of the timing data in the CNN\n",
    "peak_times = peak_times.reshape((*np.shape(peak_times),1))\n",
    "\n",
    "# overview about the important data array for later usage\n",
    "print(np.shape(peak_times)[0], \" events with 4 images each are available \\n\")\n",
    "print(\"Shape of 'event_labels': \",np.shape(event_labels))\n",
    "print(\"Shape of 'squared_training': \",np.shape(squared_training))\n",
    "print(\"Shape of 'peak_times': \",np.shape(peak_times),\"\\n\")\n",
    "\n",
    "# split into random training data (80%) and test data (20%)\n",
    "train_data, test_data, train_labels, test_labels = [], [], [], []\n",
    "random_selection = np.random.rand(np.shape(peak_times)[0]) <= 0.8\n",
    "train_data.append(peak_times[random_selection])\n",
    "test_data.append(peak_times[~random_selection])\n",
    "train_labels.append(event_labels[random_selection])\n",
    "test_labels.append(event_labels[~random_selection])\n",
    "\n",
    "# free some memory space\n",
    "del peak_times\n",
    "del event_labels\n",
    "\n",
    "# convert to numpy array and reshape \n",
    "train_data = np.array(train_data)\n",
    "train_data = train_data.reshape(np.shape(train_data[0]))\n",
    "test_data = np.array(test_data)\n",
    "test_data = test_data.reshape(np.shape(test_data[0]))\n",
    "\n",
    "train_labels = np.array(train_labels)\n",
    "train_labels = train_labels.reshape(np.shape(train_labels[0]))\n",
    "test_labels = np.array(test_labels)\n",
    "test_labels = test_labels.reshape(np.shape(test_labels[0]))\n",
    "\n",
    "train_labels_multishape = np.zeros_like(train_data)\n",
    "test_labels_multishape = np.zeros_like(test_data)\n",
    "\n",
    "len_train = np.shape(train_data)[0]\n",
    "len_test = np.shape(test_data)[0]\n",
    "\n",
    "for i in range(0,len_train):\n",
    "    train_labels_multishape[i,:,:,:] = train_labels[i]\n",
    "\n",
    "for k in range(0,len_test):\n",
    "    test_labels_multishape[k,:,:,:] = test_labels[k]\n",
    "\n",
    "# overvew about the splitting into training and test data\n",
    "print(\"Split into Training and Test Data\")\n",
    "print(\"Train data shape:\", np.shape(train_data) , \"-->\",round(100*len_train/(len_train+len_test),2),\"%\")\n",
    "print(\"Test data shape:\", np.shape(test_data), \"-->\",round(100*len_test/(len_train+len_test),2), \"%\")\n",
    "print(\"Train labels shape:\", np.shape(train_labels))\n",
    "print(\"Test labels shape:\", np.shape(test_labels))\n",
    "\n",
    "# split up different \"telescopes\" for the usage in the seperate single view CNNs (probably in the most long-winded way possible, but lets just ignore that)\n",
    "train_data_1 = train_data[:,0,:,:] \n",
    "train_data_2 = train_data[:,1,:,:] \n",
    "train_data_3 = train_data[:,2,:,:] \n",
    "train_data_4 = train_data[:,3,:,:] \n",
    "\n",
    "test_data_1 = test_data[:,0,:,:]\n",
    "test_data_2 = test_data[:,1,:,:]\n",
    "test_data_3 = test_data[:,2,:,:]\n",
    "test_data_4 = test_data[:,3,:,:]\n",
    "\n",
    "train_labels_1 = train_labels_multishape[:,0,:,:]\n",
    "train_labels_2 = train_labels_multishape[:,1,:,:]\n",
    "train_labels_3 = train_labels_multishape[:,2,:,:]\n",
    "train_labels_4 = train_labels_multishape[:,3,:,:]\n",
    "\n",
    "test_labels_1 = test_labels_multishape[:,0,:,:]\n",
    "test_labels_2 = test_labels_multishape[:,1,:,:]\n",
    "test_labels_3 = test_labels_multishape[:,2,:,:]\n",
    "test_labels_4 = test_labels_multishape[:,3,:,:]\n",
    "\n",
    "print(\"Train data 1 shape:\", np.shape(train_data_1))\n",
    "print(\"Train labels 1 shape:\", np.shape(train_labels_1))\n",
    "\n",
    "print(\"Test data 1 shape:\", np.shape(test_data_1))\n",
    "print(\"Test labels 1 shape:\", np.shape(test_labels_1))\n",
    "\n",
    "input_shape = (48, 48, 1)\n",
    "pool_size = 2\n",
    "kernel_size = 4\n",
    "\n",
    "# Define the model for the single-view CNNs\n",
    "def create_cnn_model(input_shape):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=25, kernel_size=kernel_size, activation='relu', padding='same',kernel_regularizer=regularizers.l2(reg), input_shape=input_shape,))\n",
    "    model.add(MaxPooling2D(pool_size=pool_size, padding='same'))\n",
    "\n",
    "    model.add(Dropout(rate))\n",
    "    model.add(Conv2D(filters=50, kernel_size=kernel_size, activation='relu', padding='same', kernel_regularizer=regularizers.l2(reg)))\n",
    "    model.add(MaxPooling2D(pool_size=pool_size, padding='same'))\n",
    "\n",
    "    model.add(Dropout(rate))\n",
    "    model.add(Conv2D(filters=50, kernel_size=kernel_size, activation='relu', padding='same',kernel_regularizer=regularizers.l2(reg)))\n",
    "    model.add(MaxPooling2D(pool_size=pool_size, padding='same'))\n",
    "\n",
    "    model.add(Dropout(rate))\n",
    "    model.add(Conv2D(filters=100, kernel_size=kernel_size, activation='relu', padding='same',kernel_regularizer=regularizers.l2(reg)))\n",
    "    model.add(MaxPooling2D(pool_size=pool_size, padding='same'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define the model for the combination of the previous CNNs and the final CNN for classification\n",
    "\n",
    "def run_multiview_model(models,inputs):\n",
    "\n",
    "    merged = concatenate(models)\n",
    "\n",
    "    Dropout1 = Dropout(rate)(merged)\n",
    "    Conv_merged1 = Conv2D(filters=25,kernel_size=[2,2],activation='relu',padding='same',input_shape=(48,48,1))(Dropout1)\n",
    "    MaxPool_merged1 = MaxPooling2D(pool_size=2,padding='same')(Conv_merged1)\n",
    "\n",
    "    Dropout2 = Dropout(rate)(MaxPool_merged1)\n",
    "    Conv_merged2 = Conv2D(filters=50,kernel_size=[2,2],activation='relu',padding='same',input_shape=(48,48,1))(Dropout2)\n",
    "    MaxPool_merged2 = MaxPooling2D(pool_size=2,padding='same')(Conv_merged2)\n",
    "\n",
    "    Dropout3 = Dropout(rate)(MaxPool_merged2)\n",
    "    Conv_merged3 = Conv2D(filters=100,kernel_size=[2,2],activation='relu',padding='same',input_shape=(48,48,1))(Dropout3)\n",
    "    MaxPool_merged3 = MaxPooling2D(pool_size=2,padding='same')(Conv_merged3)\n",
    "\n",
    "    Flat_merged1 = Flatten()(MaxPool_merged3)\n",
    "    Dropout4 = Dropout(rate)(Flat_merged1)\n",
    "    dense_layer_merged1 = Dense(units=100, activation='relu')(Dropout4)\n",
    "\n",
    "    Dropout5 = Dropout(rate)(dense_layer_merged1)\n",
    "    dense_layer_merged2 = Dense(units=50, activation='relu')(Dropout5)\n",
    "\n",
    "    Dropout6 = Dropout(rate)(dense_layer_merged2)\n",
    "    dense_layer_merged3 = Dense(units=1, activation='sigmoid')(Dropout6)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=dense_layer_merged3)\n",
    "    return model\n",
    "\n",
    "# Create four separate CNN models\n",
    "input_1 = Input(shape=input_shape)\n",
    "cnn_model_1 = create_cnn_model(input_shape)(input_1)\n",
    "\n",
    "input_2 = Input(shape=input_shape)\n",
    "cnn_model_2 = create_cnn_model(input_shape)(input_2)\n",
    "\n",
    "input_3 = Input(shape=input_shape)\n",
    "cnn_model_3 = create_cnn_model(input_shape)(input_3)\n",
    "\n",
    "input_4 = Input(shape=input_shape)\n",
    "cnn_model_4 = create_cnn_model(input_shape)(input_4)\n",
    "\n",
    "# include early_stopping here, to see how it changes compared to previous model designs\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=patience)\n",
    "\n",
    "model_multi = run_multiview_model([cnn_model_1, cnn_model_2, cnn_model_3, cnn_model_4],[input_1, input_2, input_3, input_4])\n",
    "model_multi.summary()\n",
    "model_multi.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy']) \n",
    "\n",
    "history = model_multi.fit([train_data[:,i,:,:] for i in range(4)],train_labels,epochs=num_epochs,batch_size=batch_size,validation_data=([test_data[:,i,:,:] for i in range(4)], test_labels), callbacks=[early_stopping])\n",
    "\n",
    "# Create the filename, which is used for saving the Accuracy and Loss plots and the history files\n",
    "str_num_epochs = '{}'.format(num_epochs)\n",
    "str_batch_size = '{}'.format(batch_size)\n",
    "str_rate = '{}'.format(rate*100)\n",
    "str_reg = '{}'.format(reg)\n",
    "\n",
    "history_name = \"history_\" + str_num_epochs + \"epochs\" + str_batch_size + \"batchsize\" + str_rate + \"rate\" + str_reg + \"reg\" + fnr + \".pkl\"\n",
    "\n",
    "# Save the history files for later usage in other scripts\n",
    "with open(history_name, 'wb') as file:\n",
    "    pickle.dump(history.history, file)\n",
    "\n",
    "# Create plots for quick overview\n",
    "fig, ax = plt.subplots(1,2, figsize = (9,3))\n",
    "ax[0].plot(history.history['accuracy'], label='Testing Data',lw=2,c=\"darkorange\")\n",
    "ax[0].plot(history.history['val_accuracy'], label = 'Validation Data',lw=2,c=\"firebrick\")\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[0].set_ylim([0.5, 1])\n",
    "ax[0].legend(loc='lower right')\n",
    "\n",
    "ax[1].plot(history.history['loss'],lw=2,c=\"darkorange\")\n",
    "ax[1].plot(history.history['val_loss'],lw=2,c=\"firebrick\")\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "\n",
    "print(\"Image created\")\n",
    "\n",
    "filename_savefig = \"Test_Cluster_\"+ str_num_epochs + \"epochs\" + str_batch_size + \"batchsize\" + str_rate + \"rate\" + fnr +\".png\"\n",
    "fig.savefig(filename_savefig, bbox_inches='tight')\n",
    "\n",
    "print(\"Image saved\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
